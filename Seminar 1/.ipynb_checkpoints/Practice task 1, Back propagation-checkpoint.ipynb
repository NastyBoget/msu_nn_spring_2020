{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семинар 1 \"Полносвязные нейронные сети\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ФИО: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит реализовать проход \"вперед\" для полносвязной нейронную сети. В дальнейшем мы реализуем процедуру обучения и научим сеть распознавать рукописные цифры.\n",
    "\n",
    "На первой лекции мы познакомились с тем, что такое нейронные сети и изучили три слоя — линейный, сигмоида и SoftMax. Из этих слоев можно составлять глубокие архитектуры и обучать их при помощи градиентного спуска. Чтобы конструировать сложные архитектуры, можно реализовать каждый тип слоя как отдельный \"кирпичик\" и затем собирать полную архитектуру как конструктор. Это мы и попробуем сделать на первом и втором семинарах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый тип слоя мы будем реализовывать при помощи класса, который будет поддерживать три функции: forward, которая будет применять функцию, реализуемую слоем, к входной матрице и backward, которая будет вычислять градиенты и step, которая будет обновлять веса. Чтобы не применять функцию к каждому объекту в отдельности, мы будем подавать на вход слою матрицу размера (N, d), где N — количество объектов, а d — размерность каждого объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=backprop.pdf width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция forward будет вычислять по $x$ значение $y$, backward — по $\\frac{\\partial L}{\\partial y}$ вычислять $\\frac{\\partial L}{\\partial x}$ и обновлять внутри себя $\\frac{\\partial L}{\\partial w}$.\n",
    "\n",
    "Важным требованием к реализации является векторизация всех слоев: все операции должны быть сведены к матричным, не должно быть циклов. Это значительно уменьшает временные затраты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1: Линейный слой\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем пример вычисления градиентов для линейного слоя: $y = Wx$, $x \\in \\mathbb{R}^{K \\times n}$, $y \\in \\mathbb{R}^{K \\times n}$, $W \\in \\mathbb{R}^{n \\times m}$, где $K$ — число объектов.\n",
    "\n",
    "Рассмотрим $L$ как функцию от выходов нейронной сети: $L = L(y_{11}, y_{12}, \\dots)$\n",
    "\n",
    "$$y_{kt} = (Wx)_{kt} = \\sum_{z=1}^{n} x_{kz}W_{zt}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x_{ij}} = \\sum_{kt} \\frac{\\partial L}{\\partial y_{kt}}\\frac{\\partial y_{kt}}{\\partial x_{ij}} = \\sum_{kt} \\frac{\\partial L}{\\partial y_{kt}}\\frac{\\partial \\sum_z x_{kz}w_{zt}}{\\partial x_{ij}}= \\sum_{t} \\frac{\\partial L}{\\partial y_{it}}\\frac{\\partial w_{jt}}{\\partial x_{ij}}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial x} = \\frac{\\partial{L}}{\\partial y}W^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Creates weights and biases for linear layer.\n",
    "        Dimention of inputs is *input_size*, of output: *output_size*.\n",
    "        '''\n",
    "        self.W = np.random.randn(input_size, output_size) * 0.1\n",
    "        self.b = np.zeros(output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Passes objects through this layer.\n",
    "        X is np.array of size (N, input_size).\n",
    "        Returns output of size (N, output_size).\n",
    "        Hint: You may need to store X for backward pass\n",
    "        '''\n",
    "        self.X = X # X - row\n",
    "        self.Y = X @ self.W + self.b # @ - matrix multiplication\n",
    "        \n",
    "        return self.Y\n",
    "    \n",
    "    def backward(self, dLdy):\n",
    "        '''\n",
    "        1. Compute dLdw and dLdx.\n",
    "        2. Store dLdw for step() call\n",
    "        3. Return dLdx\n",
    "        '''\n",
    "        self.dLdW = self.X.T.dot(dLdy)\n",
    "        self.dLdb = np.copy(dLdy)\n",
    "        # y = x * W + b\n",
    "        # dL/db = dL/dy * dy/db\n",
    "        # dL/dx = dL/dy * dy/dx\n",
    "        dLdx = dLdy @ self.W.T\n",
    "        \n",
    "        return dLdx\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        '''\n",
    "        1. Apply gradient dLdw to network:\n",
    "        w <- w - l*dLdw\n",
    "        '''\n",
    "        self.W += learning_rate * self.dLdw\n",
    "        self.b += learning_rate * self.dLdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2: Численный градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Релизуйте функцию проверки численного градиента. Для этого для каждой переменной, по которой считается градиент, надо вычислить численный градиент: $f'(x) \\approx \\frac{f(x+\\epsilon)-f(x-\\epsilon)}{2\\epsilon}$. Функция должна возвращать максимальное абсолютное отклонение аналитического градиента от численного. В качестве $\\epsilon$ рекомендуется взять $10^{-6}$. При правильной реализации максимальное отличие будет иметь порядок $10^{-8}-10^{-6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient(func, X, gradient):\n",
    "    '''\n",
    "    Computes numerical gradient and compares it with analytcal.\n",
    "    func: callable, function of which gradient we are interested. Example call: func(X)\n",
    "    X: np.array of size (n x m)\n",
    "    gradient: np.array of size (n x m)\n",
    "    Returns: maximum absolute diviation between numerical gradient and analytical.\n",
    "    '''\n",
    "    eps = 10 ** (-7)\n",
    "    ngrad_full = np.zeros_like(X)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            X[i, j] += eps\n",
    "            f_right = func(X)\n",
    "            X[i, j] -= 2 * eps\n",
    "            f_left = func(X)\n",
    "            X[i, j] += eps\n",
    "            ngrad = (f_right - f_left) / (2 * eps)\n",
    "            ngrad_full[i, j] = ngrad\n",
    "    \n",
    "    return np.max(np.abs(ngrad_full - gradient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте линейный слой при помощи реализованной функции check_gradient: $\\frac{\\partial L}{\\partial x}$ и $\\frac{\\partial L}{\\partial w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6253309809144412e-10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = np.array([[1, 1], [1, 2], [3, 3]])\n",
    "np.random.seed(0)\n",
    "X = np.array([[1, 4]])\n",
    "linear = Linear(2, 1)\n",
    "\n",
    "def func(W, X=X):\n",
    "    linear.W = W\n",
    "    results = linear.forward(X)\n",
    "    return results.flatten()[0]\n",
    "\n",
    "# L(F(x)) = F(x)\n",
    "results = linear.forward(X)\n",
    "linear.backward(np.ones_like(results))\n",
    "gradients = linear.dLdW\n",
    "check_gradient(func, linear.W, gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 3: Сигмоида"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Passes objects through this layer.\n",
    "        X is np.array of size (N, d)\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        #### Apply layer to input\n",
    "        pass\n",
    "    \n",
    "    def backward(self, dLdy):\n",
    "        '''\n",
    "        1. Compute dLdx.\n",
    "        2. Return dLdx\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте слой при помощи реализованной функции check_gradient: $\\frac{\\partial L}{\\partial x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 4: Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы увидели на семинаре, вычисление производной для связки SoftMax + Negative log-likelihood проще чем для этих двух слоев по отдельности. Поэтому мы реализуем их как один класс. Важное замечание: на проходе \"вперед\" важно воспользоваться трюком <a href=\"https://blog.feedly.com/tricks-of-the-trade-logsumexp/\">log-sum-exp</a>, чтобы не столкнуться с численными неустойчивостями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLoss:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Applies Softmax operation to inputs and computes NLL loss\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        #### (Hint: No code is expected here, just joking)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        '''\n",
    "        Passes objects through this layer.\n",
    "        X is np.array of size (N, C), where C is the number of classes\n",
    "        y is np.array of size (N), contains correct labels\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        #### Apply layer to input\n",
    "        pass\n",
    "    \n",
    "    def backward(self):\n",
    "        '''\n",
    "        Note that here dLdy = 1 since L = y\n",
    "        1. Compute dLdx\n",
    "        2. Return dLdx\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте слой при помощи реализованной функции check_gradient: $\\frac{\\partial L}{\\partial x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 5, нейронная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть \"кирпичики\", мы можем написать класс, который будет собирать всю сеть вместе "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, modules):\n",
    "        '''\n",
    "        Constructs network with *modules* as its layers\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        #### YOUR CODE HERE\n",
    "        #### Apply layers to input\n",
    "        for module in self.modules:\n",
    "            X = module.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, dLdy):\n",
    "        '''\n",
    "        dLdy here is a gradient from loss function\n",
    "        '''\n",
    "        #### YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def step(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 6, обучение на простых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data.npz')\n",
    "X, y = data['arr_0'], data['arr_1']\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите архитектуру вида 2 -> 10 -> 10 -> 3:\n",
    "* Linear(2, 10)\n",
    "* Sigmoid()\n",
    "* Linear(10, 10)\n",
    "* Sigmoid()\n",
    "* Linear(10, 3)\n",
    "\n",
    "В качестве функции потерь используйте NLLLoss.\n",
    "1. Создайте сеть, в цикле запускайте forward, backward, step (используйте learning rate 0.005). \n",
    "2. Нарисуйте график сходимости (величина NLL после каждого обновления).\n",
    "3. Нарисуйте разделяющую поверхность\n",
    "4. Попробуйте подобрать темп обучения. Как меняется сходимость?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличная визуализация: http://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Подоберите темп обучения. Как меняется сходимость? Нарисуйте график оптимального значения функции потерь для различных значений learning_rate\n",
    "* Решите поставленную выше задачу как задачу регрессии с MSE. Изменилась ли разделяющая поверхность?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
