# msu_nn_spring_2020

## Лекции и семинары по курсу "Нейронные сети в машинном обучении" 

[Видеолекции](https://www.youtube.com/playlist?list=PLrCZzMib1e9oOGNLh6_d65HyfdqlJwTQP)

1) Основы нейронных сетей

2) Детали обучения нейронных сетей

3) Библиотеки для глубинного обучения

4) Сверточные нейронные сети

5) Улучшение сходимости нейросетей

6) Архитектуры нейронных сетей

7) Методы оптимизации

8) Рекуррентные нейронные сети

9) Обработка естественного языка

10) Современные модели для NLP

11) Соперничающие сети (GAN)

12) Вариационные автоэнкодеры. Нейронные сети для искусства

13) Обучение с подкреплением 1

### Рубежный контроль № 1

#### Вопросы:

* Теоретический минимум

  * Произношение слов accuracy, bias, variable, function, Байесовский
  
  * Функции потерь для классификации и регрессии
  
  * Проблемы обучения глубинных нейронных сетей
  
  * Граф вычислений
  
  * Задачи на подсчет матричных производных
  
  * Операция свертки
  
  * Операция пулинга
  
  * Постановка задачи оптимизации
  
  * Постановка задачи регрессии
  
  * Постановка задачи классификации
  
  * Способы валидации модели
  
  * Особенности работы с изображениями
  
  * Идея transfer learning
  
#### Билеты
  
  * Алгоритм обратного распространения ошибки для графов вычислений
  
  * Модель линейной регрессии и ее решение
  
  * Сверточные нейронные сети
  
  * Dropout, Batch normalization
  
  * GD, SGD, Momentum, NAG
  
  * Adagrad, RMSprop, Adadelta, Adam
  
  * Инициализация сетей: Xavier, He, ортогональная инициализация
  
  * ResNet
  
  * VGG, Network in Network
  
  * Inception
  
### Рубежный контроль № 2

#### Вопросы:

* Теоретический минимум

  * Архитектура LSTM ячейки

  * Attention

  * Self-Attention

  * Идея соперничающих сетей

  * KL дивергенция

  * Основные понятия обучения с подкреплением: агент, среда, стратегия, награда

  * Уравнения Беллмана

  * Обратное распространение ошибки сквозь время

#### Билеты

* Метод главных компонент

* Рекуррентные сети. Проблемы с прохождением градиента. Обратное распространение ошибки сквозь время

* LSTM и GRU сети

* Word2Vec: CBOW и Skip-gram

* Иерархический и дифференцированный софтмакс

* Задача обучения языковой модели. Seq2Seq

* Transformer, BERT

* Autoencoder и Denoising Autoencoder.

* Соперничающие сети и генеративные автокодировщики

* Вариационный автокодировщик. Вариационная нижняя оценка

* Интерпретация обученных сетей. Saliency map. Deep dream и artistic style

* Задача обучения с подкреплением. Алгоритм Policy iteration

* Q-learning. Deep Q-learning.

* Multi-armed bandits

* MCTS

* Alpha go
